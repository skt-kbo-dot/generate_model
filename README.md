# control-a-video

# KBO DOT MODEL

## git branch ì „ëµ

> ë©”ì¸ ë¸Œëœì¹˜ : main, develop

> ë³´ì¡° ë¸Œëœì¹˜ : feature
>
> > feaure ë¸Œëœì¹˜ ëª…ëª… ë°©ì‹ì€ feature/[ê¸°ëŠ¥ì´ë¦„]

> ë¦´ë¦¬ì¦ˆ ë¸Œëœì¹˜ : release
>
> > dev ->release -> main

> í•«í”½ìŠ¤ ë¸Œëœì¹˜ : hotfix
>
> > main -> hotfix -> main

&nbsp;

## Commit, PRì‹œ

- Rule 1 : Commitì–‘ì‹ì€ ì•„ë˜ë¥¼ ë”°ë¦…ë‹ˆë‹¤.
- Rule 2 : ì œëª©ì€ ì˜ì–´ë¡œ, ë³¸ë¬¸ì€ í•œê¸€ë¡œ ì‘ì„±í•˜ì—¬ ì£¼ì„¸ìš”.

```
# <íƒ€ì…>: <ì œëª©>

##### ì œëª©ì€ ìµœëŒ€ 50 ê¸€ìê¹Œì§€ë§Œ ì…ë ¥ ############## -> |


# ë³¸ë¬¸ì€ ìœ„ì— ì‘ì„±
######## ë³¸ë¬¸ì€ í•œ ì¤„ì— ìµœëŒ€ 72 ê¸€ìê¹Œì§€ë§Œ ì…ë ¥ ########################### -> |

# ê¼¬ë¦¿ë§ì€ ì•„ë˜ì— ì‘ì„±: ex) #ì´ìŠˆ ë²ˆí˜¸

# --- COMMIT END ---
# <íƒ€ì…> ë¦¬ìŠ¤íŠ¸
#   feat    : ê¸°ëŠ¥ (ìƒˆë¡œìš´ ê¸°ëŠ¥)
#   fix     : ë²„ê·¸ (ë²„ê·¸ ìˆ˜ì •)
#   refactor: ë¦¬íŒ©í† ë§
#   style   : ìŠ¤íƒ€ì¼ (ì½”ë“œ í˜•ì‹, ì„¸ë¯¸ì½œë¡  ì¶”ê°€: ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì— ë³€ê²½ ì—†ìŒ)
#   docs    : ë¬¸ì„œ (ë¬¸ì„œ ì¶”ê°€, ìˆ˜ì •, ì‚­ì œ)
#   test    : í…ŒìŠ¤íŠ¸ (í…ŒìŠ¤íŠ¸ ì½”ë“œ ì¶”ê°€, ìˆ˜ì •, ì‚­ì œ: ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì— ë³€ê²½ ì—†ìŒ)
#   chore   : ê¸°íƒ€ ë³€ê²½ì‚¬í•­ (ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì • ë“±)
# ------------------
#     ì œëª© ì²« ê¸€ìë¥¼ ëŒ€ë¬¸ìë¡œ
#     ì œëª©ì€ ëª…ë ¹ë¬¸ìœ¼ë¡œ
#     ì œëª© ëì— ë§ˆì¹¨í‘œ(.) ê¸ˆì§€
#     ì œëª©ê³¼ ë³¸ë¬¸ì„ í•œ ì¤„ ë„ì›Œ ë¶„ë¦¬í•˜ê¸°
#     ë³¸ë¬¸ì€ "ì–´ë–»ê²Œ" ë³´ë‹¤ "ë¬´ì—‡ì„", "ì™œ"ë¥¼ ì„¤ëª…í•œë‹¤.
#     ë³¸ë¬¸ì— ì—¬ëŸ¬ì¤„ì˜ ë©”ì‹œì§€ë¥¼ ì‘ì„±í•  ë• "-"ë¡œ êµ¬ë¶„
# ------------------
```

```
ex)
docs: Update README

ê°€ë…ì„±ì´ ë” ì¢‹ì€ commit ë©”ì‹œì§€ë¡œ ì—…ë°ì´íŠ¸ í•˜ì˜€ìŠµë‹ˆë‹¤.
```

## ì£¼ì„ Convention

- Rule 3 : í•¨ìˆ˜, í´ë˜ìŠ¤ ë‹¨ìœ„ë¡œ ì•„ë˜ ì£¼ì„ í˜•ì‹ì„ ë”°ë¼ì£¼ì„¸ìš”.
  - descriptionì€ ì „ì²´ì ì¸ ê¸°ëŠ¥, ë™ì‘ì´ ë³µì¡í•˜ë‹¤ë©´ ìì„¸í•˜ê²Œ ì¨ì£¼ì„¸ìš”.

```
 /**
  *@author BeomGi-Lee, jeongiun@naver.com
  *@date 2023-08-11
  *@description ìƒë‹¨ì— ê³ ì •ì ìœ¼ë¡œ ìœ„ì¹˜í•˜ëŠ” Header
  *             ë¡œê³ , Main Navigator, ê²€ìƒ‰ì°½,
  *             KR/EN ë²„íŠ¼, Side Navigator í¬í•¨
  */
```

- Rule 4 : í•¨ìˆ˜ ì•ˆì— í° ì»´í¬ë„ŒíŠ¸ ë‹¨ìœ„ë¡œ í•œì¤„ì£¼ì„ í˜¹ì€ returnë¬¸ ë‚´ì— ì£¼ì„ì„ ë‹¬ì•„ì£¼ì„¸ìš”.

```
// return ë¬¸ ì™¸
// í•œì¤„ ì£¼ì„

{/* return ë¬¸ ë‚´ */}
{/* ë©”ë‰´, ê²€ìƒ‰ì°½, ì–¸ì–´ë²„íŠ¼ */}
{/* ì‚¬ì´ë“œ ë©”ë‰´ */}
```

## Code Convention

Rule 5 : ê¸°ë³¸ì ì¸ Conventionì€ VS Code í™•ì¥ Prettierì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

- íŒŒì¼ ì €ì¥ ì‹œ ì„œì‹ì´ ìë™ ì§€ì •ë˜ë„ë¡ Format On Save ê¸°ëŠ¥ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.

## Code Review

Rule 6 : PRëœ Codeë¥¼ Reviewí•˜ì‹œê³  ì´ìƒ ì—†ì–´ë³´ì´ë©´ LGTM(Look Good To Me) ëŒ“ê¸€ì„ ë‚¨ê²¨ì£¼ì„¸ìš”.  
Rule 7 : ë” ì¢‹ì€ ë°©ë²•ì´ë‚˜ ìˆ˜ì •í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ì€ ë¶€ë¶„ ëŒ“ê¸€ë¡œ ë‚¨ê²¨ì£¼ì„¸ìš”.  
Rule 8 : Codeì— ê´€ë ¨ëœ ë¶€ë¶„ë§Œ ì§€ì í•˜ì—¬ ì£¼ì„¸ìš”.  
Rule 9 : LGTM 3ëª… ì¦‰ 3ëª…ì´ìƒì˜ Code Reviewë¥¼ í†µê³¼í•˜ë©´ Mergeí•©ë‹ˆë‹¤.

## Issue Convention

Rule 10 : ì´ìŠˆ ì‘ì„±ì‹œ ì•„ë˜ì˜ í˜•ì‹ì„ ë”°ë¼ì£¼ì„¸ìš”.

```
## ğŸ“’ ì´ìŠˆ ë‚´ìš©
> "ì´ìŠˆ ë‚´ìš© ì‘ì„±"

## ğŸ“‘ ìƒì„¸ ë‚´ìš©
1. "ìƒì„¸ ë‚´ìš© 1"
2. "ìƒì„¸ ë‚´ìš© 2"

## âœ”ï¸ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ìƒì„¸ ë‚´ìš© 1.
- [ ] ìƒì„¸ ë‚´ìš© 2.
```

<!-- <img src="basketball.gif" width="256"> -->

<!-- Official Implementation of ["Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models"](https://arxiv.org/abs/2305.13840)

- [Project Page](https://controlavideo.github.io)
- [Online Demo](https://huggingface.co/spaces/wf-genius/Control-A-Video)

Similar to Controlnet, We otain the condition maps from another video, and we support three kinds of control maps at this time. -->

## Model

|                               depth control                                |                        depth control                        |                     depth control                      |
| :------------------------------------------------------------------------: | :---------------------------------------------------------: | :----------------------------------------------------: |
| <img src="videos/abstract painting.gif" width="260"><br> abstract painting | <img src="videos/ë¥´ë„¤ìƒìŠ¤.gif" width="200"><br> renaissance | <img src="videos/gogh.gif" width="200"><br> gogh-style |

# Setup

The model has been tesed in torch version: `1.13.1+cu117`, simply run

```
pip3 install -r requirements.txt
```

# Usage

## 1. Quick Use

We provide a demo for quick testing in this repo, simply running:

```
python3 inference.py --prompt "a bear walking through stars, artstation" --input_video bear.mp4 --control_mode depth
```

Args:

- `--input_video`: path of input video(mp4 format).
- `--num_sample_frames`: nums of frames to generate. (recommend > 8).
- `--each_sample_frame`: sampling frames for each time. (for auto-regressive generateion.)
- `--sampling_rate`: skip sampling from the input video.

- `--control_mode`: allows for different control, currently support **`canny`, `depth`, `hed`**. (you need to download the weight of **hed** annotator from [link](https://huggingface.co/wf-genius/controlavideo-hed/resolve/main/hed-network.pth) and put it in work space.)
- `--video_scale`: guidance scale of video consistency, borrows from GEN-1. (don't be too large, 1~2 work well, set 0 to disable it.)
- `--init_noise_thres`: the propoed threshold of residual-based noise init. (range from 0 to 1, larger value leads to more smooth but may introduce artifacts.)

- `--inference_step, --guidance_scale, --height, --width, --prompt`: same as other T2I model.

If the automatic downloading not work, the models weights can be downloaded from: [depth_control_model](https://huggingface.co/wf-genius/controlavideo-depth), [canny_control_model](https://huggingface.co/wf-genius/controlavideo-canny), [hed_control_model](https://huggingface.co/wf-genius/controlavideo-hed).

## 2. Auto-Regressive Generation

Our model firstly generates the first frame. Once We get the first frame, we generate the subsquent frames conditioned on the first frame. Thus, it will allow our model to generate longer videos auto-regressive. (This operation is still under experiment and it may collaspe after 3 or 4 iterations.)

```
python3 inference.py --prompt "a bear walking through stars, artstation" --input_video bear.mp4 --control_mode depth --num_sample_frames 16 --each_sample_frame 8
```

Note that `num_sample_frames` should be multiple of `each_sample_frame`.

## Replace the 2d model (Experimentally)

Since we freeze the 2d model, you can replace it with any other model based on `stable-diffusion-v1-5` to generate custom-style videos.

```
state_dict_path = os.path.join(pipeline_model_path, "unet", "diffusion_pytorch_model.bin")
state_dict = torch.load(state_dict_path, map_location="cpu")
video_controlnet_pipe.unet.load_2d_state_dict(state_dict=state_dict)    # reload 2d model.
```

# Acknowledgement

This repository borrows heavily from [Diffusers](https://github.com/huggingface/diffusers), [ControlNet](https://github.com/lllyasviel/ControlNet), [Tune-A-Video](https://github.com/showlab/Tune-A-Video), thanks for open-sourcing! This work was done in Bytedance, thanks for the cooperators!
